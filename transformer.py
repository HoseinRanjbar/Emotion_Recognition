"""
This was heavily inspired from http://nlp.seas.harvard.edu/2018/04/03/attention.html
"""

from multiprocessing import Value
from turtle import mode
import torch
import torch.nn as nn
import torchvision
import numpy as np
import torch.nn.functional as F
import math, copy
from torch.autograd import Variable
import matplotlib.pyplot as plt

#Import customizable CNN
from tools import mb2

#For debugging
torch.set_printoptions(threshold=5000)

#A helper function for producing N identical layers
def clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


#Self-Attention mechanism
def ScaledDotProductAttention(query, key, value, mask=None, dropout=None):

    "Compute 'Scaled Dot Product Attention'"
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) \
             / math.sqrt(d_k)

    #src_mask=(batch, 1, 1, max_seq) #NOTE: this is like the tutorials but it is weird!
    #trg_mask = (batch, 1, max_seq, max_seq)
    #score=(batch, n_heads, Seq, Seq)

    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    p_attn = F.softmax(scores, dim = -1)

    if dropout is not None:
        p_attn = dropout(p_attn)

    output = torch.matmul(p_attn, value)

    return output #(Batch, n_heads, Seq, d_k)


class MultiHeadedAttention(nn.Module):
    def __init__(self, n_heads, n_units, dropout=0.3):
        """
        n_heads: the number of attention heads
        n_units: the number of output units
        dropout: probability of DROPPING units
        """
        super(MultiHeadedAttention, self).__init__()

        # This sets the size of the keys, values, and queries (self.d_k) to all 
        # be equal to the number of output units divided by the number of heads.
        #d_k = dim of key for one head
        self.d_k = n_units // n_heads

        #This requires the number of n_heads to evenly divide n_units.
        #NOTE: nb of n_units (hidden_size) must be a multiple of 6 (n_heads) 
        assert n_units % n_heads == 0
        #n_units represent total of units for all the heads

        self.n_units = n_units
        self.n_heads = n_heads

        self.linears = clones(nn.Linear(n_units, n_units), 4)

        self.dropout = nn.Dropout(p=dropout)


    def forward(self, query, key, value, mask=None):

        if mask is not None:
            # Same mask applied to all h heads.
            mask = mask.unsqueeze(1)

        nbatches = query.size(0)

        # 1) Do all the linear projections in batch from d_model => h x d_k
        query, key, value = \
                    [l(x).view(nbatches, -1, self.n_heads, self.d_k).transpose(1, 2)
                    for l, x in zip(self.linears, (query, key, value))]


        # 2) Apply attention on all the projected vectors in batch.
        x = ScaledDotProductAttention(query, key, value, mask=mask,
                                 dropout=self.dropout)

        # 3) "Concat" using a view and apply a final linear.
        x = x.transpose(1, 2).contiguous() \
             .view(nbatches, -1, self.n_heads * self.d_k)

        z = self.linears[-1](x)

        #(batch_size, seq_len, self.n_units)
        return z
#--------------------------------------------------------------------------
#-------------------------------------------------------------------------------
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class MaskedConv1D(nn.Module):
    """
    Masked 1D convolution. Interface remains the same as Conv1d.
    Only support a sub set of 1d convs
    """
    def __init__(
        self,
        in_channels,
        out_channels,
        kernel_size,
        stride=1,
        padding=0,
        dilation=1,
        groups=1,
        bias=True,
        padding_mode='zeros'
    ):
        super().__init__()
        # element must be aligned
        assert (kernel_size % 2 == 1) and (kernel_size // 2 == padding)
        # stride
        self.stride = stride
        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size,
                              stride, padding, dilation, groups, bias, padding_mode)
        # zero out the bias term if it exists
        if bias:
            torch.nn.init.constant_(self.conv.bias, 0.)

    def forward(self, x, mask):
        # x: batch size, feature channel, sequence length,
        # mask: batch size, 1, sequence length (bool)
        B, C, T = x.size()
        # input length must be divisible by stride
        assert T % self.stride == 0

        # conv
        out_conv = self.conv(x)
        # compute the mask
        if self.stride > 1:
            # downsample the mask using nearest neighbor
            out_mask = F.interpolate(
                mask.to(x.dtype), size=out_conv.size(-1), mode='nearest'
            )
        else:
            # masking out the features
            out_mask = mask.to(x.dtype)

        # masking the output, stop grad to mask
        out_conv = out_conv * out_mask.detach()
        out_mask = out_mask.bool()
        return out_conv, out_mask
#----------------------------------------------------------------------------------
class LocalMaskedMHCA(nn.Module):
    """
    Local Multi Head Conv Attention with mask

    Add a depthwise convolution within a standard MHA
    The extra conv op can be used to
    (1) encode relative position information (relacing position encoding);
    (2) downsample the features if needed;
    (3) match the feature channels

    Note: With current implementation, the downsampled feature will be aligned
    to every s+1 time step, where s is the downsampling stride. This allows us
    to easily interpolate the corresponding positional embeddings.

    The implementation is fairly tricky, code reference from
    https://github.com/huggingface/transformers/blob/master/src/transformers/models/longformer/modeling_longformer.py
    """

    def __init__(
        self,
        n_embd,          # dimension of the output features
        n_head,          # number of heads in multi-head self-attention
        window_size,     # size of the local attention window
        n_qx_stride=1,   # dowsampling stride for query and input
        n_kv_stride=1,   # downsampling stride for key and value
        attn_pdrop=0.0,  # dropout rate for the attention map
        proj_pdrop=0.0,  # dropout rate for projection op
        use_rel_pe=False # use relative position encoding
    ):
        super().__init__()
        assert n_embd % n_head == 0
        self.n_embd = n_embd
        self.n_head = n_head
        self.n_channels = n_embd // n_head
        self.scale = 1.0 / math.sqrt(self.n_channels)
        self.window_size = window_size
        self.window_overlap  = window_size // 2
        # must use an odd window size
        assert self.window_size > 1 and self.n_head >= 1
        self.use_rel_pe = use_rel_pe

        # conv/pooling operations
        assert (n_qx_stride == 1) or (n_qx_stride % 2 == 0)
        assert (n_kv_stride == 1) or (n_kv_stride % 2 == 0)
        self.n_qx_stride = n_qx_stride
        self.n_kv_stride = n_kv_stride

        # query conv (depthwise)
        kernel_size = self.n_qx_stride + 1 if self.n_qx_stride > 1 else 3
        stride, padding = self.n_kv_stride, kernel_size // 2
        self.query_conv = MaskedConv1D(
            self.n_embd, self.n_embd, kernel_size,
            stride=stride, padding=padding, groups=self.n_embd, bias=False
        )
        self.query_norm = LayerNorm(self.n_embd)

        # key, value conv (depthwise)
        kernel_size = self.n_kv_stride + 1 if self.n_kv_stride > 1 else 3
        stride, padding = self.n_kv_stride, kernel_size // 2
        self.key_conv = MaskedConv1D(
            self.n_embd, self.n_embd, kernel_size,
            stride=stride, padding=padding, groups=self.n_embd, bias=False
        )
        self.key_norm = LayerNorm(self.n_embd)
        self.value_conv = MaskedConv1D(
            self.n_embd, self.n_embd, kernel_size,
            stride=stride, padding=padding, groups=self.n_embd, bias=False
        )
        self.value_norm = LayerNorm(self.n_embd)

        # key, query, value projections for all heads
        # it is OK to ignore masking, as the mask will be attached on the attention
        self.key = nn.Conv1d(self.n_embd, self.n_embd, 1)
        self.query = nn.Conv1d(self.n_embd, self.n_embd, 1)
        self.value = nn.Conv1d(self.n_embd, self.n_embd, 1)

        # regularization
        self.attn_drop = nn.Dropout(attn_pdrop)
        self.proj_drop = nn.Dropout(proj_pdrop)

        # output projection
        self.proj = nn.Conv1d(self.n_embd, self.n_embd, 1)

        # relative position encoding
        if self.use_rel_pe:
            self.rel_pe = nn.Parameter(
                torch.zeros(1, 1, self.n_head, self.window_size + 1))
            trunc_normal_(self.rel_pe, std=(2.0 / self.n_embd)**0.5)

    @staticmethod
    def _chunk(x, window_overlap):
        """convert into overlapping chunks. Chunk size = 2w, overlap size = w"""
        # x: B x nh, T, hs
        # non-overlapping chunks of size = 2w -> B x nh, T//2w, 2w, hs
        x = x.view(
            x.size(0),
            x.size(1) // (window_overlap * 2),
            window_overlap * 2,
            x.size(2),
        )

        # use `as_strided` to make the chunks overlap with an overlap size = window_overlap
        chunk_size = list(x.size())
        chunk_size[1] = chunk_size[1] * 2 - 1
        chunk_stride = list(x.stride())
        chunk_stride[1] = chunk_stride[1] // 2

        # B x nh, #chunks = T//w - 1, 2w, hs
        return x.as_strided(size=chunk_size, stride=chunk_stride)

    @staticmethod
    def _pad_and_transpose_last_two_dims(x, padding):
        """pads rows and then flips rows and columns"""
        # padding value is not important because it will be overwritten
        x = nn.functional.pad(x, padding)
        x = x.view(*x.size()[:-2], x.size(-1), x.size(-2))
        return x

    @staticmethod
    def _mask_invalid_locations(input_tensor, affected_seq_len):
        beginning_mask_2d = input_tensor.new_ones(affected_seq_len, affected_seq_len + 1).tril().flip(dims=[0])
        beginning_mask = beginning_mask_2d[None, :, None, :]
        ending_mask = beginning_mask.flip(dims=(1, 3))
        beginning_input = input_tensor[:, :affected_seq_len, :, : affected_seq_len + 1]
        beginning_mask = beginning_mask.expand(beginning_input.size())
        # `== 1` converts to bool or uint8
        beginning_input.masked_fill_(beginning_mask == 1, -float("inf"))
        ending_input = input_tensor[:, -affected_seq_len:, :, -(affected_seq_len + 1) :]
        ending_mask = ending_mask.expand(ending_input.size())
        # `== 1` converts to bool or uint8
        ending_input.masked_fill_(ending_mask == 1, -float("inf"))

    @staticmethod
    def _pad_and_diagonalize(x):
        """
        shift every row 1 step right, converting columns into diagonals.
        Example::
              chunked_hidden_states: [ 0.4983,  2.6918, -0.0071,  1.0492,
                                       -1.8348,  0.7672,  0.2986,  0.0285,
                                       -0.7584,  0.4206, -0.0405,  0.1599,
                                       2.0514, -1.1600,  0.5372,  0.2629 ]
              window_overlap = num_rows = 4
             (pad & diagonalize) =>
             [ 0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000
               0.0000,  -1.8348,  0.7672,  0.2986,  0.0285, 0.0000,  0.0000
               0.0000,  0.0000, -0.7584,  0.4206, -0.0405,  0.1599, 0.0000
               0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629 ]
        """
        total_num_heads, num_chunks, window_overlap, hidden_dim = x.size()
        # total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1).
        x = nn.functional.pad(
            x, (0, window_overlap + 1)
        )
        # total_num_heads x num_chunks x window_overlap*window_overlap+window_overlap
        x = x.view(total_num_heads, num_chunks, -1)
        # total_num_heads x num_chunks x window_overlap*window_overlap
        x = x[:, :, :-window_overlap]
        x = x.view(
            total_num_heads, num_chunks, window_overlap, window_overlap + hidden_dim
        )
        x = x[:, :, :, :-1]
        return x

    def _sliding_chunks_query_key_matmul(
        self, query, key, num_heads, window_overlap
    ):
        """
        Matrix multiplication of query and key tensors using with a sliding window attention pattern. This implementation splits the input into overlapping chunks of size 2w with an overlap of size w (window_overlap)
        """
        # query / key: B*nh, T, hs
        bnh, seq_len, head_dim = query.size()
        batch_size = bnh // num_heads
        assert seq_len % (window_overlap * 2) == 0
        assert query.size() == key.size()

        chunks_count = seq_len // window_overlap - 1

        # B * num_heads, head_dim, #chunks=(T//w - 1), 2w
        chunk_query = self._chunk(query, window_overlap)
        chunk_key = self._chunk(key, window_overlap)

        # matrix multiplication
        # bcxd: batch_size * num_heads x chunks x 2window_overlap x head_dim
        # bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim
        # bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap
        diagonal_chunked_attention_scores = torch.einsum(
            "bcxd,bcyd->bcxy", (chunk_query, chunk_key))

        # convert diagonals into columns
        # B * num_heads, #chunks, 2w, 2w+1
        diagonal_chunked_attention_scores = self._pad_and_transpose_last_two_dims(
            diagonal_chunked_attention_scores, padding=(0, 0, 0, 1)
        )

        # allocate space for the overall attention matrix where the chunks are combined. The last dimension
        # has (window_overlap * 2 + 1) columns. The first (window_overlap) columns are the window_overlap lower triangles (attention from a word to
        # window_overlap previous words). The following column is attention score from each word to itself, then
        # followed by window_overlap columns for the upper triangle.
        diagonal_attention_scores = diagonal_chunked_attention_scores.new_empty(
            (batch_size * num_heads, chunks_count + 1, window_overlap, window_overlap * 2 + 1)
        )

        # copy parts from diagonal_chunked_attention_scores into the combined matrix of attentions
        # - copying the main diagonal and the upper triangle
        diagonal_attention_scores[:, :-1, :, window_overlap:] = diagonal_chunked_attention_scores[
            :, :, :window_overlap, : window_overlap + 1
        ]
        diagonal_attention_scores[:, -1, :, window_overlap:] = diagonal_chunked_attention_scores[
            :, -1, window_overlap:, : window_overlap + 1
        ]
        # - copying the lower triangle
        diagonal_attention_scores[:, 1:, :, :window_overlap] = diagonal_chunked_attention_scores[
            :, :, -(window_overlap + 1) : -1, window_overlap + 1 :
        ]

        diagonal_attention_scores[:, 0, 1:window_overlap, 1:window_overlap] = diagonal_chunked_attention_scores[
            :, 0, : window_overlap - 1, 1 - window_overlap :
        ]

        # separate batch_size and num_heads dimensions again
        diagonal_attention_scores = diagonal_attention_scores.view(
            batch_size, num_heads, seq_len, 2 * window_overlap + 1
        ).transpose(2, 1)

        self._mask_invalid_locations(diagonal_attention_scores, window_overlap)
        return diagonal_attention_scores

    def _sliding_chunks_matmul_attn_probs_value(
        self, attn_probs, value, num_heads, window_overlap
    ):
        """
        Same as _sliding_chunks_query_key_matmul but for attn_probs and value tensors. Returned tensor will be of the
        same shape as `attn_probs`
        """
        bnh, seq_len, head_dim = value.size()
        batch_size = bnh // num_heads
        assert seq_len % (window_overlap * 2) == 0
        assert attn_probs.size(3) == 2 * window_overlap + 1
        chunks_count = seq_len // window_overlap - 1
        # group batch_size and num_heads dimensions into one, then chunk seq_len into chunks of size 2 window overlap

        chunked_attn_probs = attn_probs.transpose(1, 2).reshape(
            batch_size * num_heads, seq_len // window_overlap, window_overlap, 2 * window_overlap + 1
        )

        # pad seq_len with w at the beginning of the sequence and another window overlap at the end
        padded_value = nn.functional.pad(value, (0, 0, window_overlap, window_overlap), value=-1)

        # chunk padded_value into chunks of size 3 window overlap and an overlap of size window overlap
        chunked_value_size = (batch_size * num_heads, chunks_count + 1, 3 * window_overlap, head_dim)
        chunked_value_stride = padded_value.stride()
        chunked_value_stride = (
            chunked_value_stride[0],
            window_overlap * chunked_value_stride[1],
            chunked_value_stride[1],
            chunked_value_stride[2],
        )
        chunked_value = padded_value.as_strided(size=chunked_value_size, stride=chunked_value_stride)

        chunked_attn_probs = self._pad_and_diagonalize(chunked_attn_probs)

        context = torch.einsum("bcwd,bcdh->bcwh", (chunked_attn_probs, chunked_value))
        return context.view(batch_size, num_heads, seq_len, head_dim)

    def forward(self, x, mask):
        # x: batch size, feature channel, sequence length,
        # mask: batch size, 1, sequence length (bool)
        x = x.transpose(1,2)

        B, C, T = x.size()

        # step 1: depth convolutions
        # query conv -> (B, nh * hs, T')
        q, qx_mask = self.query_conv(x, mask)
        q = self.query_norm(q)
        # key, value conv -> (B, nh * hs, T'')
        k, kv_mask = self.key_conv(x, mask)
        k = self.key_norm(k)
        v, _ = self.value_conv(x, mask)
        v = self.value_norm(v)

        # step 2: query, key, value transforms & reshape
        # projections
        q = self.query(q)
        k = self.key(k)
        v = self.value(v)
        
        # (B, nh * hs, T) -> (B, nh, T, hs)
        q = q.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)
        k = k.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)
        v = v.view(B, self.n_head, self.n_channels, -1).transpose(2, 3)
        # view as (B * nh, T, hs)
        q = q.view(B * self.n_head, -1, self.n_channels).contiguous()
        k = k.view(B * self.n_head, -1, self.n_channels).contiguous()
        v = v.view(B * self.n_head, -1, self.n_channels).contiguous()

        # step 3: compute local self-attention with rel pe and masking
        q *= self.scale
        # chunked query key attention -> B, T, nh, 2w+1 = window_size

        att = self._sliding_chunks_query_key_matmul(
            q, k, self.n_head, self.window_overlap)

        # rel pe
        if self.use_rel_pe:
            att += self.rel_pe
        # kv_mask -> B, T'', 1
        inverse_kv_mask = torch.logical_not(
            kv_mask[:, :, :, None].view(B, -1, 1))
        # 0 for valid slot, -inf for masked ones
        float_inverse_kv_mask = inverse_kv_mask.type_as(q).masked_fill(
            inverse_kv_mask, -1e4)
        # compute the diagonal mask (for each local window)
        diagonal_mask = self._sliding_chunks_query_key_matmul(
            float_inverse_kv_mask.new_ones(size=float_inverse_kv_mask.size()),
            float_inverse_kv_mask,
            1,
            self.window_overlap
        )
        att += diagonal_mask

        # ignore input masking for now
        att = nn.functional.softmax(att, dim=-1)
        # softmax sometimes inserts NaN if all positions are masked, replace them with 0
        att = att.masked_fill(
            torch.logical_not(kv_mask.squeeze(1)[:, :, None, None]), 0.0)
        att = self.attn_drop(att)

        # step 4: compute attention value product + output projection
        # chunked attn value product -> B, nh, T, hs
        out = self._sliding_chunks_matmul_attn_probs_value(
            att, v, self.n_head, self.window_overlap)
        # transpose to B, nh, hs, T -> B, nh*hs, T
        out = out.transpose(2, 3).contiguous().view(B, C, -1)
        # output projection + skip connection
        #out = self.proj_drop(self.proj(out)) * qx_mask.to(out.dtype)
        out = (self.proj(out)) * qx_mask.to(out.dtype)
        #return out, qx_mask

        out = out.transpose(1,2)
        return out
#----------------------------------------------------------------------------------
#Extract embeddings from hand images
#Hand representations

#TO DO: to customize like 2D_embeddings
class HandExtractor(nn.Module):
    def __init__(self, n_units=1280, pretrained=False, network_type='mb2', channels=1):
        super(HandExtractor, self).__init__()

        #self.network = torchvision.models.mobilenet_v2(pretrained=pretrained)
        self.network = mb2.mobilenet_v2(pretrained=pretrained, channels=channels)

        #Drop FC layer
        modules = list(self.network.children())[:-1]
        self.network = nn.Sequential(*modules)

        self.n_units= n_units

    #Input (batch, hands, seq, 3, 64, 64)
    #Input 1 hand (batch, seq, 3, 64, 64)
    def forward(self, x):
        batch, seq, _, _, _ = x.shape

        x = x.view(batch*seq, x.shape[-3], x.shape[-2], x.shape[-1])
        emb = self.network(x)

        #Apply AVG POOL if we have a feature map
        if(len(emb.shape) > 2):
            emb =  emb.mean(3).mean(2)

        #Reshape embeddings as expected from transformer block
        emb = emb.view(batch, -1, self.n_units)

        return emb

#----------------------------------------------------------------------------------

#Extract embeddings from images
#Full frame representations
class src_2Dembeddings(nn.Module):
    def __init__(self, n_units, pretrained=True, image_size=224, network_type='mb2', channels=3):

        super(src_2Dembeddings, self).__init__()

        self.n_units = n_units
        self.network_type = network_type

        self.position = PositionalEncoding(n_units, 0.3)

        #Use mb2 for image embeddings
        #TO DO: customize other arch to make them able train on grayscale
        if(self.network_type=='mb2'):
            #self.network = torchvision.models.mobilenet_v2(pretrained=pretrained)
            self.network = mb2.mobilenet_v2(channels=channels, pretrained=pretrained)

        elif (self.network_type=='alexnet'):
            self.network = torchvision.models.alexnet(pretrained=pretrained)

        elif (self.network_type=='resnet'):
            self.network = torchvision.models.resnet18(pretrained=pretrained)

        elif (self.network_type=='convnext'):
            self.network = torchvision.models.convnext_small(pretrained=pretrained)

        elif (self.network_type=='resnet50'):
            self.network = torchvision.models.resnet50(pretrained=pretrained)

        elif (self.network_type=='efficientnet_b7'):
            self.network = torchvision.models.efficientnet_b7(pretrained=pretrained)

        elif (self.network_type=='vgg11'):
            self.network = torchvision.models.vgg11(pretrained=pretrained)

        elif (self.network_type=='resnext'):
            self.network = torchvision.models.resnext50_32x4d(pretrained=pretrained)

        elif(self.network_type=='wide_resnet'):
            self.network = torchvision.models.wide_resnet50_2(pretrained=pretrained)

        else:
            print('No supported architecture!!')
            quit(0)

        #Drop FC layer
        modules = list(self.network.children())[:-1]
        self.network = nn.Sequential(*modules)

        #Placeholder for gradients
        self.gradients = None

    #hook for gradientrs of the activations
    def activations_hook(self, grad):
        self.gradients = grad

    def forward(self, x):

        batch_size = x.shape[0]
        seq_len = x.shape[1]

        #Reshape to join seq size w/ batch
        x = x.view(batch_size*seq_len, x.shape[2], x.shape[3], x.shape[4])

        #Extract embeddings for full frames
        feature_map = self.network(x)
        #print(feature_map.size())

        #register the hook
        #if(feature_map.requires_grad):
            #print('here')
         #   feature_map.register_hook(self.activations_hook)
            #sd

        #Apply AVG POOL if we have a feature map
        if(len(feature_map.shape) > 2):
            frame_embeddings =  feature_map.mean(3).mean(2)

        #Reshape embeddings as expected from transformer block
        frame_embeddings = frame_embeddings.view(batch_size, -1, self.n_units)

        #image emb = (batch, seq_length, n_units)
        return frame_embeddings, feature_map, self.gradients


#The positional encoding 'tags' each element of an input sequence with a code that 
#identifies it's position (i.e. time-step).
#We use sine and cosine functions to encode positions

class PositionalEncoding(nn.Module):
    def __init__(self, n_units, dropout, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        # Compute the positional encodings once in log space.
        pe = torch.zeros(max_len, n_units)
        position = torch.arange(0., max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0., n_units, 2).float() *
                             -(math.log(10000.0) / n_units))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):

        #Positianal encoding (batch, seq_length, emb_size)
        pos = Variable(self.pe[:, :x.size(1)], requires_grad=False)
        return self.dropout(x + pos)

##########

#Encoder stack is composed of a multi-head self attention + norm layer + FF + norm
#See figure's left side from original transformner network

class EncoderStack(nn.Module):

    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderStack, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        #2 skip+norm layers: one after muli-head attention, the second after FF
        self.sublayer = clones(ResidualSkipWithLayerNorm(size, dropout), 2)
        self.size = size

    def forward(self, x, mask=None, hand_emb=None):

        #Pass hand embeddings as query, full frame embeddings as context
        if(type(hand_emb) != type(None)):
            x = self.sublayer[0](hand_emb, None, lambda hand_emb: self.self_attn(hand_emb, x, x, mask))

        #If there is no query: query, keys and vector are the same
        else:
            x = self.sublayer[0](x, None, lambda x: self.self_attn(x, mask))

        return self.sublayer[1](x, None, self.feed_forward)

##You want them to have different parameters; the point of stacking
#multiple encoders is that each encoder can transform the data independently
#of the others, resulting in a more expressive model.
#If they have the same parameters, then it's the same as having 1 encoder.

class Encoder(nn.Module):
    def __init__(self, layer,N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNormalization(layer.size)

    def forward(self, x, hand_emb, mask):

        #Pass the input (and mask) through each layer in turn.
        for i in range(0, len(self.layers)):
            x = self.layers[i](x, mask, hand_emb)

            if(type(hand_emb) != type(None)):
                break

        #(batch, seq, n_units)
        return self.norm(x)


#Full transformer network architecture (encoder + decoder + output)
class FullTransformer(nn.Module):
    def __init__(self,encoder, src_emb, output, position, window_size, num_classes, classifier_hidden_dim, hidden_size,dropout):
        super(FullTransformer, self).__init__()
        self.encoder = encoder
        self.src_emb = src_emb
        self.output_layer = output
        self.hand_emb = HandExtractor()
        self.position = position
        self.window_size = window_size
        self.dropout = dropout

        # Classifier module
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, classifier_hidden_dim),
            nn.ReLU(),
            nn.Dropout(self.dropout),
            nn.Linear(classifier_hidden_dim, num_classes)
        )

        #Placeholder for gradients
        self.gradients = None
        #Placeholder for activations
        self.activations = None

     #method for the gradient extraction
    def get_activations_gradient(self):
        return self.gradients

    #method for the activation extraction
    def get_activations(self):
        return self.activations

    #hook for gradientrs of the activations
    def activations_hook(self, grad):
        self.gradients = grad

    def encode(self, src_emb, hand_emb, src_mask):
        return self.encoder(src_emb, hand_emb, src_mask)

    #Call this when training
    def forward(self, src, src_mask, rel_mask=None, hand_seqs=None):

        #Use normal mask
        if(type(rel_mask) == type(None)):
            rel_mask = src_mask

        #Get context seq emb
        src_emb, f_map, grad = self.src_emb(src)

        #register the hook
        if(f_map.requires_grad):
            f_map.register_hook(self.activations_hook)

        self.activations = f_map

        if src_emb.size()[1] % self.window_size !=0:
            n = src_emb.size()[1] // self.window_size
            m=((n+1)*self.window_size)-src_emb.size()[1]
            src_emb = torch.nn.functional.pad(src_emb,(0,0,0,m),mode='constant',value=0)
            
            src_mask = torch.nn.functional.pad(src_mask,(0,m),mode='constant',value=0)

        #(batch, seq_length, feature_dim)
        src_emb = self.position(src_emb)
        src_emb = self.encode(src_emb, None, src_mask)

        # Pooling across time dimension
        pooled_features = src_emb.mean(dim=1)  # Shape: [batch_size, hidden_dim]

        # Pass through classifier
        class_logits = self.classifier(pooled_features)  # Shape: [batch_size, num_classes]

        # Apply softmax to get probabilities
        class_probs = F.softmax(class_logits, dim=-1)  # Apply softmax across the class dimension

        #Get hand seq emb
        if(type(hand_seqs) != type(None)):

            hand_emb = self.hand_emb(hand_seqs)

            hand_emb = self.position(hand_emb)
            hand_emb = self.encode(hand_emb, None, src_mask)

            #Context-Hand attention
            #Combine hand emb with its context
            #Use relative masking
            comb_emb = self.encode(src_emb, hand_emb, rel_mask)

            hand_out = self.output_layer(hand_emb)
            comb_out = self.output_layer(comb_emb)

        else:
            comb_out = None
            hand_out = None

        return comb_out, class_logits, hand_out


#Create the full model
def make_model(num_classes, n_stacks=2, n_units=512, n_heads=8, window_size = 10, d_ff=2048, dropout=0.3, image_size=224, pretrained=True,
                emb_type='2d', emb_network='mb2', full_pretrained=None, hand_pretrained=None, freeze_cnn=False, channels=3,
                classifier_hidden_dim=256):

    c = copy.deepcopy
    #attn = MultiHeadedAttention(n_heads, n_units, dropout)
    attn = LocalMaskedMHCA(n_embd=n_units,n_head=n_heads,window_size=window_size,attn_pdrop=dropout,proj_pdrop=dropout,use_rel_pe=True)
    ff = PositionWise(n_units, d_ff, dropout)
    position = PositionalEncoding(n_units, dropout)

    model = FullTransformer(
        encoder=Encoder(EncoderStack(n_units, c(attn), c(ff), dropout), n_stacks),
        src_emb=src_2Dembeddings(n_units, pretrained, image_size, network_type=emb_network, channels=channels),
        output= None,
        position= c(position),
        window_size = window_size,
        num_classes=num_classes,
        classifier_hidden_dim=classifier_hidden_dim,
        hidden_size= n_units,
        dropout=dropout)

    #Load pretrained CNNs (trained on same dataset)
    if(full_pretrained):
        model.src_emb.load_state_dict(torch.load(full_pretrained))
        print("Full frame CNN pretrained weights successfully loaded..")

    if(hand_pretrained):
        model.hand_emb.load_state_dict(torch.load(hand_pretrained))
        print("Hand CNN pretrained weights succesfully loaded..")

    #Initialize parameters with Glorot/xavier. (except image/hand embeddings that are init by imagenet weights)
    for name, p in model.named_parameters():

        if p.dim() > 1 and 'src_emb' not in name:
            nn.init.xavier_uniform_(p)

    return model


#########################


#----------------------------------------------------------------------------------
#Layer normalization

class LayerNormalization(nn.Module):
    "layer normalization, as in: https://arxiv.org/abs/1607.06450"
    def __init__(self, features, eps=1e-6):
        super(LayerNormalization, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

class LayerNorm(nn.Module):
    """
    LayerNorm that supports inputs of size B, C, T
    """
    def __init__(
        self,
        num_channels,
        eps = 1e-5,
        affine = True,
        device = None,
        dtype = None,
    ):
        super().__init__()
        factory_kwargs = {'device': device, 'dtype': dtype}
        self.num_channels = num_channels
        self.eps = eps
        self.affine = affine

        if self.affine:
            self.weight = nn.Parameter(
                torch.ones([1, num_channels, 1], **factory_kwargs))
            self.bias = nn.Parameter(
                torch.zeros([1, num_channels, 1], **factory_kwargs))
        else:
            self.register_parameter('weight', None)
            self.register_parameter('bias', None)

    def forward(self, x):
        assert x.dim() == 3
        assert x.shape[1] == self.num_channels

        # normalization along C channels
        mu = torch.mean(x, dim=1, keepdim=True)
        res_x = x - mu
        sigma = torch.mean(res_x**2, dim=1, keepdim=True)
        out = res_x / torch.sqrt(sigma + self.eps)

        # apply weight and bias
        if self.affine:
            out *= self.weight
            out += self.bias

        return out

class ResidualSkipWithLayerNorm(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, size, dropout):
        super(ResidualSkipWithLayerNorm, self).__init__()
        self.norm = LayerNormalization(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, query, sublayer):
        "Apply residual connection to any sublayer with the same size."
        attn_x = self.dropout(sublayer(self.norm(x)))
        return x + attn_x

#A simple Feed forward MLP
class PositionWise(nn.Module):

    def __init__(self, n_units, d_ff, dropout=0.1):
        super(PositionWise, self).__init__()
        self.w_1 = nn.Linear(n_units, d_ff)
        self.w_2 = nn.Linear(d_ff, n_units)
        self.dropout = nn.Dropout(dropout)
        #self.activation = nn.GELU()

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))
        #return self.w_2(self.dropout(self.activation(self.w_1(x))))



